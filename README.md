# emoji-learn

## Whats is emoji-learn? ğŸ¤”

emoji-learns is a simple library to create neural networks - in [Emojicode](https://www.emojicode.org)!  
How can this not be awesome??

## Motivation
n/a

## Look and Feel
So...how does machine learning in Emojicode actually look like?? Here are some screenshots to save you an extra click to my GitHub repo:

![Screenshot 1](images/screenshot01.png)

![Screenshot 2](images/screenshot02.png)

Yeah, it really is someting.

## Prerequisites ğŸ“
Emojicode version 0.9+ is required. Go [here](https://github.com/emojicode/emojicode) to find out how to install it.  
If you want to find out how to use Emojicode go through the excellent [language reference](https://www.emojicode.org/docs/reference/) or [package index](https://www.emojicode.org/docs/packages/).  

After you cloned the repository you at first have to build the numlol package. Go to packages/numlol and run the command

```
emojicodec -p numlol numlol.emojic
```

This will generate the numlol package.The emoji-learn package depends on this. So afterwards go to packages/emoji-learn and run

```
emojicodec -p emoji-learn -S /path/to/GitRepo/emoji-learn/packages emoji-learn.emojic
```


## Documentation ğŸ¤“

## numlol ğŸ’¯
Machine learning and especially neural nets rely heavily on the use of linear algebra (LinAlg) - we need to add, substract and multiply matrices and vectors. Unfortunately, the only data structure available in the standard Emojicode package that comes close to what we need are lists. 

```
ğŸ’­ A list of integers
ğŸ¨ 1 2 3 4 5ğŸ†
```

Because of this emoji-learn ships with a dedicated LinAlg-library called numlol (not to be confused with the imposters from [numpy](https://numpy.org/)).  
  
Creating a new numlol array is as simple as one-two-three, just hand over an embedded list to its constructor.

```
ğŸ’­ Create a 3*2 matrix and store it in the variable 'matrix'
  ğŸ†•ğŸğŸ†• ğŸ¨ğŸ¨ 1.0 2.0 ğŸ†
            ğŸ¨ 3.0 4.0 ğŸ†
            ğŸ¨ 5.0 6.0 ğŸ†ğŸ† â—â¡ï¸ matrix
            
ğŸ’­ Create a 3*1 vector and store it in the variable 'vector'
  ğŸ†•ğŸğŸ†• ğŸ¨ğŸ¨ 1.0 ğŸ†
            ğŸ¨ 3.0 ğŸ†
            ğŸ¨ 5.0 ğŸ†ğŸ† â—â¡ï¸ vector
```

In the numlol package all kind of matrix and vector operations are implemented:
(Note: most methods are implementes as static methods so that the method name itself is followed by ğŸ‡ğŸ)  

Want to add two vectors? No problem!

```
ğŸ’­ Elementwise addition of two arrays with the same shape
ğŸ‹ğŸ‡ğŸ array01 array02â—â¡ï¸ sum_array
```

You suddenly need to transpose a matrix? Here you go!

```
ğŸ’­ Transpose array
ğŸ”ğŸ‡ğŸ array0â—â¡ï¸ transposed_array
```

It is neccesary to perform a matrix-matrix/matrix-vector multiplication? Sure!
```
ğŸ’­ Do matrix-matrix/matrix-vector multiplication
ğŸ¥ğŸ‡ğŸ array01 array02â—â¡ï¸ multiplied_array
```

It also contains some more complicated functions we will need for machine learning later, e.g. the mean squared error function (ğŸ‘¬) or the logistic activation function (ğŸ“¸).   

It is also easy to print arrays to the console.

```
ğŸ’­ Print array to console
ğŸ“  arrayâ—
```
This will produce an ouput similar to the following

![Screenshot 4](images/screenshot04.png)

### emoji-learn ğŸ“š
Ah, machine learning! The stuff all the cool kids do. So let's dive right into it.  

First of all we need data. A lot of it. At the moment the only way to get data in our net is via a .csv file. So your data should look like this

![Screenshot 3](images/screenshot03.png)

Then this file content can be transfered into a numlol array with just one method call

```
ğŸ’­ Read .csv file into numlol array
ğŸ¦‹ğŸ‡ğŸ•¸ ğŸ”¤datasets/iris.csvğŸ”¤â— â¡ï¸ data
```

This method does not just read the data, but already normalizes the features and performs a one-hot encoding on the labels.
You don't have any datasets you can try out? No problem, we got you! There are already some toy datasets included. 

```
ğŸ’­ Read the iris dataset into a numlol array
ğŸŒºğŸ‡ğŸ•¸ â— â¡ï¸ data

ğŸ’­ Read the Pima Indians diabetes dataset into a numlol array
ğŸ›¶ğŸ‡ğŸ•¸ â— â¡ï¸ data

ğŸ’­ Read the sonar dataset into a numlol array
ğŸš¢ğŸ‡ğŸ•¸ â— â¡ï¸ data
```
Unfortunately an Emojicode method cannot return multiple values. Therefore 'data' is a value type that holds the features ans the labels. There are two methods to access them both.

```
ğŸ’­ Access features and labels from the 'data' value type
ğŸ¬ dataâ— â¡ï¸ X
ğŸ¦ˆ dataâ— â¡ï¸ y
```

Next step is to split the data in a train and a test set. There is also a handy method for this.

```
ğŸ’­ Split in train and test data
ğŸ’­ 0.333 means that (about) a third of all the data will be in the test data
ğŸ…ğŸ‡ğŸ•¸ X y 0.333â— â¡ï¸ train_test_data
```

Afterwards we can create our net. The constructor for the net expects just one parameter, a list with integers where each integer defines the number of neurons in each layer. 

```
ğŸ’­ Create a neural net. This one has:
ğŸ’­ - 60 neurons in the input layer
ğŸ’­ - 12 neurons in the hidden layer
ğŸ’­ - 2 neurons in the output layer
ğŸ’­ The number of hidden layers is NOT limited to one.
ğŸ’­ It can be arbitrary (big)
ğŸ†•ğŸ•¸ğŸ†• ğŸ¨ 60 12 2ğŸ† â—â¡ï¸ neural_net
```

And now the magic takes place, we can train the net!

```
ğŸ’­ Train the net with the given data over 100 epochs with a learning rate of 0.1
ğŸ¦„ neural_net X_train y_train 100 0.1â—
```

To evaluate the performance we can finally calculate and print the accuracy.

```
ğŸ’­ Train the net with the given data over 100 epochs with a learning rate of 0.1
  ğŸ¦ neural_net X_test y_testâ— â¡ï¸ accuracy_test
  ğŸ˜€ğŸª ğŸ”¤Accuracy test data: ğŸ”¤ ğŸ”¡accuracy_testâœ–ï¸100 2â—ğŸ”¤%ğŸ”¤ ğŸªâ—
```

This method can then be used to make predictions. 

```
ğŸ’­ Get the 3rd sample from the array X, make a prediction and print the result
ğŸ“  ğŸ¦• neural_net ğŸ¥«ğŸ‡ğŸ X 3â—â—â—
```

## Performance
So, how does an Emojicode net performs on different datasets? I made a 10 fold crossvalidation training each net over 100 epochs, and here are the results (accuracies):

X | Iris dataset | Sonar dataset | Pima Indian diabetes dataset
--- | --- | --- | ---
Train | 0.966 +/- 0.011 | 0.9986 +/- 0.0030 | 0.833 +/- 0.015
Test | 0.98 +/- 0.023 | 0.784 +/- 0.029 | 0.746 +/- 0.026

Wow, not too bad!
